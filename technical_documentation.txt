=======================================
Technical Documentation: BussGPT Agent
=======================================

Version: 1.0
Date: 2025-04-25

1. Project Overview
-------------------
This project implements a multi-tool hierarchical retrieval agent using the LangChain framework. The agent is designed to answer queries, primarily financial ones related to company earnings transcripts, by leveraging a sequence of specialized tools. It starts with broad context gathering, narrows down to specific categories (e.g., companies), and finally analyzes individual documents for detailed answers and evidence. The agent utilizes Anthropic's Claude language models (specifically Haiku) for analysis and decision-making within its tools and core logic.

2. Core Components
------------------
The system comprises several key Python components located primarily within the `langchain_tools` directory:

2.1. Hierarchical Retrieval Agent (`langchain_tools/agent.py`):
    - Implements the main `HierarchicalRetrievalAgent` class.
    - Uses LangChain's ReAct (Reasoning and Acting) framework (`create_react_agent`).
    - Manages the overall workflow, deciding which tool to call next based on the query and previous observations.
    - Integrates with the `ToolFactory` to get initialized tools.
    - Utilizes `AgentState` (`langchain_tools/state_manager.py`) to track conversation progress, collected evidence, document IDs, etc.
    - Includes custom output parsing (`langchain_tools/output_parser.py`) with fix-up logic for LLM formatting issues.
    - Handles logging via `AgentLogger` (`langchain_tools/logger.py`).
    - The core agent logic is driven by the prompt defined in `langchain_tools/agent_config.py`.

2.2. Tools (`langchain_tools/tool*.py`):
    - **Tool Factory (`langchain_tools/tool_factory.py`):** Creates instances of the analysis tools, wrapping them with validation logic and ensuring consistent interfaces. It parses single-string inputs from the agent into multiple arguments required by the underlying tool functions.
    - **Department Tool (`langchain_tools/tool1_department.py`):** The first tool called. Analyzes high-level department summaries (e.g., TECH sector) stored in MongoDB to provide broad context, identify relevant categories (company tickers), and potentially answer very high-level queries. Uses an LLM for analysis.
    - **Category Tool (`langchain_tools/tool2_category.py`):** Called after the Department Tool identifies a relevant category. Analyzes category-specific summaries (e.g., for AMZN) stored in MongoDB. Aims to answer queries based on the summary and, critically, identify relevant document IDs from the summary for deeper analysis. Returns an empty list if no *provided* document IDs seem relevant. Uses an LLM for analysis and document ID selection based on strict prompt instructions (`config.py`).
    - **Document Tool (`langchain_tools/tool3_document.py`):** Called if the Category Tool returns relevant document IDs. Retrieves the full text of specified documents (earnings transcripts) from MongoDB. Uses an LLM to analyze the document content in the context of the query, extract evidence (quotes), and provide a detailed, evidence-based answer.

2.3. Configuration (`langchain_tools/config.py`, `langchain_tools/agent_config.py`, `langchain_tools/tool_prompts_config.json`):
    - **`config.py`:** Contains functions to load tool configurations (`tool_prompts_config.json`), format prompts for each tool (e.g., `format_category_prompt`), and utility functions like `sanitize_json_response` for cleaning LLM outputs.
    - **`agent_config.py`:** Defines the core agent behavior:
        - `AGENT_PROMPT`: The main ReAct prompt template guiding the agent's reasoning and tool usage sequence (includes strict rules for tool progression).
        - `TOOL_DESCRIPTIONS`: Textual descriptions of each tool provided to the agent's LLM.
        - `AGENT_CONFIG`: Basic LangChain agent executor settings (max iterations, verbosity).
    - **`tool_prompts_config.json`:** (Likely) Stores the specific prompt templates used internally by each tool's LLM analysis step (loaded by functions in `config.py`).

2.4. Database (MongoDB):
    - Assumed to be running locally (`mongodb://localhost:27017/`).
    - Database Name: `earnings_transcripts`
    - Key Collections:
        - `documents`: Stores the raw text content of earnings call transcripts, likely indexed by a unique document ID.
        - `category_summaries`: Stores pre-generated summaries for specific categories (e.g., company tickers like AMZN), including metadata and lists of associated document IDs.
        - `department_summaries`: Stores pre-generated summaries for broader departments/sectors (e.g., TECH), potentially linking to categories.

3. Directory Structure
----------------------
/Users/saadahmed/Desktop/Apps/BussGPT/
├── .env                  # Environment variables (e.g., API keys) - NOT COMMITTED
├── .gitignore            # Specifies intentionally untracked files
├── langchain_tools/      # Core agent and tool logic
│   ├── agent.py
│   ├── tool_factory.py
│   ├── tool1_department.py
│   ├── tool2_category.py
│   ├── tool3_document.py
│   ├── config.py
│   ├── agent_config.py
│   ├── output_parser.py
│   ├── state_manager.py
│   ├── logger.py
│   ├── orchestrator.py       # (Appears related but less explored in recent debug)
│   ├── multi_agent_system.py # (Appears related but less explored in recent debug)
│   ├── tool_prompts_config.json
│   └── __init__.py
├── config/               # Older config structure? (Potentially deprecated)
│   ├── agent.json
│   └── prompts.json
├── data/                 # Source data (e.g., from Dataverse)
│   └── ...
├── dataverse_files/      # Unzipped source data
│   └── ...
├── tests/                # Unit/integration tests
│   └── ...
├── database_backup*/     # Backup directories from scripts
│   └── ...
├── venv/                 # Python virtual environment - NOT COMMITTED
├── requirements.txt      # Python dependencies (Generated below)
├── cat_dept_call.py      # Script for testing category/department tools and agent
├── run_single_query.py   # Script for running individual queries through agent
├── summarize_category.py # Script to generate category summaries
├── generate_department_summary.py # Script to generate department summaries
├── import_summary_to_db.py # Script to load summaries into MongoDB
├── database_cleanup.py   # Utility script for DB operations
├── database_backup.py    # Utility script for DB operations
└── ... (other scripts and generated output files)

4. Setup Instructions
---------------------
1.  **Clone Repository:** Obtain the project code (e.g., `git clone <repository_url>`).
2.  **Install MongoDB:** Download and install MongoDB Community Edition from the official website if not already installed. Ensure the MongoDB server (`mongod`) is running.
3.  **Create Python Environment:**
    ```bash
    cd /path/to/BussGPT
    python3 -m venv venv
    source venv/bin/activate
    ```
4.  **Install Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
5.  **Set Environment Variables:** Create a `.env` file in the project root directory and add necessary API keys:
    ```dotenv
    ANTHROPIC_API_KEY="sk-..."
    # Add other keys if required by different components
    ```
6.  **Prepare Data:**
    *   Place the source data (e.g., earnings transcripts from `dataverse_files/Transcripts`) in the expected location if not already present.
    *   Run the necessary data processing and summarization scripts to populate the MongoDB database. The exact sequence might be:
        *   (Initial script to load raw transcripts into `documents` collection - *needs verification*)
        *   Run `python3 summarize_category.py` (or similar) to generate category summaries.
        *   Run `python3 generate_department_summary.py` (or similar) to generate department summaries.
        *   Run `python3 import_summary_to_db.py` (or similar) to load these summaries into the respective MongoDB collections.
        *   *Note: The exact data loading/processing workflow needs confirmation by examining these scripts.*

5. Dependencies
---------------
5.1. Python Packages (`requirements.txt`):
    ```
    aiohappyeyeballs==2.6.1
    aiohttp==3.11.17
    aiosignal==1.3.1
    annotated-types==0.7.0
    anthropic==0.49.0
    anyio==4.9.0
    attrs==25.3.0
    certifi==2025.1.31
    charset-normalizer==3.4.1
    defusedxml==0.7.1
    distro==1.9.0
    dnspython==2.7.0
    filelock==3.18.0
    frozenlist==1.6.0
    fsspec==2025.3.2
    h11==0.14.0
    httpcore==1.0.8
    httpx==0.28.1
    huggingface-hub==0.30.2
    idna==3.10
    iniconfig==2.1.0
    jiter==0.9.0
    jsonpatch==1.33
    jsonpointer==3.0.0
    langchain==0.3.23
    langchain-anthropic==0.3.12
    langchain-core==0.3.54
    langchain-text-splitters==0.3.8
    langgraph==0.3.31
    langgraph-checkpoint==2.0.24
    langgraph-prebuilt==0.1.8
    langgraph-sdk==0.1.63
    langsmith==0.1.147
    multidict==6.4.3
    numpy==1.26.4
    orjson==3.10.16
    ormsgpack==1.9.1
    packaging==24.2
    pluggy==1.5.0
    propcache==0.3.1
    pydantic==2.11.3
    pydantic_core==2.33.1
    pymongo==4.6.1
    pytest==8.3.5
    python-dotenv==1.1.0
    PyYAML==6.0.2
    regex==2024.11.6
    requests==2.32.3
    requests-toolbelt==1.0.0
    sniffio==1.3.1
    SQLAlchemy==2.0.40
    tenacity==8.5.0
    tiktoken==0.9.0
    tokenizers==0.21.1
    tqdm==4.67.1
    typing-inspection==0.4.0
    typing_extensions==4.13.2
    urllib3==2.4.0
    uuid==1.30
    xxhash==3.5.0
    yarl==1.20.0
    zstandard==0.23.0
    ```
5.2. External Dependencies:
    *   MongoDB (Tested with version compatible with PyMongo 4.6.1)

6. Data Flow / Workflow
-----------------------
1.  **Query Input:** User submits a query (e.g., via `cat_dept_call.py` or `run_single_query.py`).
2.  **Agent Invocation:** `HierarchicalRetrievalAgent.query()` method is called.
3.  **State Reset:** Agent state (`AgentState`) is reset.
4.  **Agent Executor:** LangChain's `AgentExecutor` starts the ReAct loop.
5.  **LLM Thought:** The agent's core LLM reasons based on `AGENT_PROMPT` and current state.
6.  **Tool Selection:** Agent decides to use `department_tool`.
7.  **Action Formatting:** Agent formats `Action: department_tool` and `Action Input: <query>`.
8.  **Tool Execution:** `department_tool` runs (fetches summaries, calls its LLM via `tool1_department.py`).
9.  **Observation:** Result from `department_tool` (analysis, suggested category) is returned to the agent.
10. **LLM Thought:** Agent analyzes the observation, identifies the category (e.g., AMZN).
11. **Tool Selection:** Agent decides to use `category_tool`.
12. **Action Formatting:** Agent formats `Action: category_tool` and `Action Input: <query>, category=AMZN`.
13. **Tool Execution:** `tool_factory.py` parses the input string. `category_tool` runs (fetches category summary, calls its LLM via `tool2_category.py` using prompt from `config.py`). The LLM selects relevant document IDs *only* from the list provided in the summary.
14. **Observation:** Result from `category_tool` (answer, relevant document IDs, confidence) is returned.
15. **LLM Thought:** Agent analyzes the observation.
16. **CRITICAL RULE Check:** Agent checks if `relevant_doc_ids` is non-empty.
    *   **If YES:** Agent MUST select `document_tool` next.
    *   **If NO (as in recent tests):** Agent proceeds based on confidence/completeness. It might decide it needs `document_tool` anyway (e.g., if confidence low) or proceed to Final Answer.
17. **Tool Selection (if needed):** Agent selects `document_tool`.
18. **Action Formatting:** Agent formats `Action: document_tool` and `Action Input: <query>, doc_ids=[...]`.
19. **Tool Execution:** `tool_factory.py` parses input. `document_tool` runs (fetches documents from MongoDB, calls its LLM via `tool3_document.py`).
20. **Observation:** Result from `document_tool` (detailed answer, evidence) is returned.
21. **LLM Thought:** Agent analyzes the final evidence.
22. **Final Answer:** Agent formats `Final Answer: <final_result>`.
23. **Output Formatting:** `_format_final_response` creates the final result dictionary (converting sets to lists).
24. **Logging:** Final response and state are logged via `logger.py`.
25. **Return:** The final result dictionary is returned.

7. Key Scripts and Usage
------------------------
*   **`cat_dept_call.py`:** A primary script used for testing. It first calls the category tool directly and then runs a predefined query through the full `HierarchicalRetrievalAgent`. Useful for end-to-end debugging.
    ```bash
    # Ensure ANTHROPIC_API_KEY is set
    source venv/bin/activate
    python3 cat_dept_call.py
    ```
*   **`run_single_query.py`:** Allows running arbitrary queries through the agent.
    ```bash
    # Ensure ANTHROPIC_API_KEY is set
    source venv/bin/activate
    python3 run_single_query.py "Your query here"
    ```
*   **Data Processing Scripts (`summarize*.py`, `generate*.py`, `import*.py`):** Used for the initial data loading and summarization pipeline to populate MongoDB. Examine these scripts for the exact data preprocessing steps.

8. Potential Improvements / Future Work
---------------------------------------
*   **Document ID Reliability:** Investigate why the `category_tool` LLM sometimes hallucinates or selects non-existent document IDs. This might involve improving the quality/content of category summaries or further refining the category tool prompt. Ensure the document IDs listed in summaries actually exist in the `documents` collection.
*   **Database Schema/Indexing:** Optimize MongoDB schema and add indexes for faster lookups, especially for summaries and documents.
*   **Error Handling:** Enhance error handling within tools and the agent loop for more graceful failures (e.g., handle MongoDB connection errors, specific API errors).
*   **Prompt Engineering:** Continuously refine prompts for the agent core and internal tool LLMs for better accuracy, formatting compliance, and efficiency.
*   **LLM Evaluation/Selection:** Experiment with different LLMs (e.g., Claude Sonnet/Opus, GPT-4) to see impact on performance and adherence to instructions.
*   **State Management:** Refine the `AgentState` dataclass and state transitions for clarity and robustness.
*   **Logging:** Improve logging detail and structure for easier debugging. Add options for different log levels. Consider structured logging (JSON).
*   **Testing:** Expand unit and integration tests in the `tests/` directory to cover more scenarios and edge cases.
*   **Data Loading Pipeline:** Document and potentially streamline the data loading/summarization pipeline. 