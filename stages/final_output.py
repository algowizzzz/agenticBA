# Logic for the Final Output stage (LLMChain for answer) 
import logging
import json
from typing import Dict

from langchain.chains import LLMChain
from langchain_core.language_models import BaseChatModel
from langchain_core.prompts import BasePromptTemplate

logger = logging.getLogger(__name__)

def generate_final_answer(query: str, reasoning_text: str, llm: BaseChatModel, prompt: BasePromptTemplate, 
                          execution_summary: str = None) -> str:
    """
    Generates the final, user-facing answer based on the reasoning.
    Returns a JSON-formatted string with the answer and a summary of steps.

    Args:
        query: The original user query.
        reasoning_text: The reasoning text generated by the reasoning stage.
        llm: The language model instance.
        prompt: The prompt template for the final output stage.
        execution_summary: The execution summary from the execution stage.

    Returns:
        A JSON-formatted string containing the final answer and steps summary.
    """
    logger.info(f"[Final Output] Generating final answer for query: {query[:100]}...")
    final_answer = "Error: Failed to generate final answer."
    
    # Create response object to hold both answer and steps summary
    response_object = {
        "answer": "",
        "steps_summary": ""
    }
    
    # Process the execution_summary to create a formatted steps summary if available
    if execution_summary:
        # Create a formatted text summary instead of a list
        steps_text = "Main Orchestration Steps:\n"
        
        # Add Guardrails step (always happens)
        steps_text += "• Guardrails Stage: Validated the query against safety guidelines\n"
        
        # Add Planning step (always happens)
        steps_text += "• Planning Stage: Generated a plan based on the query\n"
        
        # Add Plan Confirmation step (always happens)
        steps_text += "• Plan Confirmation: Presented the plan and received approval\n"
        
        # Add Execution step with details
        tools_used = []
        tools_detail = ""
        
        # Extract tool information and SQL queries from execution summary
        steps = execution_summary.split('\n')
        
        # Count steps with "Step" prefix
        step_count = len([s for s in steps if s.strip() and s.startswith("Step ")])
        steps_text += f"• Execution Stage: Processed {step_count} steps sequentially\n"
        
        # Add reasoning and final output steps
        steps_text += "• Reasoning Stage: Analyzed results to formulate the response\n"
        steps_text += "• Final Output Stage: Generated the JSON response with answer and steps summary\n"
        
        # Extract SQL queries directly from step strings
        sql_queries = []
        for step in steps:
            # Identify tools used
            if "FinancialSQL:" in step:
                if "FinancialSQL" not in tools_used:
                    tools_used.append("FinancialSQL")
            elif "CCRSQL:" in step:
                if "CCRSQL" not in tools_used:
                    tools_used.append("CCRSQL")
            elif "EarningsCallSummary:" in step:
                if "EarningsCallSummary" not in tools_used:
                    tools_used.append("EarningsCallSummary")
            elif "FinancialNewsSearch:" in step:
                if "FinancialNewsSearch" not in tools_used:
                    tools_used.append("FinancialNewsSearch")
            elif "DirectAnswer:" in step:
                if "DirectAnswer" not in tools_used:
                    tools_used.append("DirectAnswer")
            
            # Extract SQL queries
            if "SQL: `" in step:
                try:
                    sql_start = step.find("SQL: `") + 6
                    sql_end = step.find("`", sql_start)
                    if sql_start > 6 and sql_end > sql_start:
                        sql_query = step[sql_start:sql_end]
                        sql_queries.append(sql_query)
                except Exception as e:
                    logger.warning(f"Failed to extract SQL query: {e}")
        
        # Add Tools Called section
        if tools_used:
            steps_text += "\nTools Called:\n"
            for tool in tools_used:
                steps_text += f"• {tool}\n"
                
                # Add SQL query details for SQL tools
                if tool in ["FinancialSQL", "CCRSQL"] and sql_queries:
                    for query in sql_queries:
                        steps_text += f"  • SQL Query: {query}\n"
                
                # Add details for specific tools
                if tool == "EarningsCallSummary":
                    steps_text += "  • EarningsCallSummary internally used:\n"
                    steps_text += "    - category_tool: Fetched high-level category summaries\n"
                    steps_text += "    - metadata_lookup_tool: Identified relevant document IDs\n"
                    steps_text += "    - document_content_analysis_tool: Analyzed specific documents\n"
        
        response_object["steps_summary"] = steps_text
    else:
        response_object["steps_summary"] = "No execution steps available"

    # Check if this is a direct response from DirectAnswer tool
    if reasoning_text.startswith("DIRECT_RESPONSE:"):
        logger.info("[Final Output] Processing direct response from reasoning stage.")
        # Extract the direct response (everything after the prefix)
        direct_answer = reasoning_text.replace("DIRECT_RESPONSE:", "", 1).strip()
        if direct_answer:
            response_object["answer"] = direct_answer
            return json.dumps(response_object, indent=2)
        else:
            logger.warning("[Final Output] Empty direct response received.")
            response_object["answer"] = "I apologize, but I couldn't generate a response to your request."
            return json.dumps(response_object, indent=2)

    # Check if reasoning stage itself returned an error
    if reasoning_text.startswith("Error:"):
        logger.warning(f"[Final Output] Reasoning stage failed: {reasoning_text}. Cannot generate final answer.")
        response_object["answer"] = f"I apologize, but I encountered an error during the reasoning process: {reasoning_text}"
        return json.dumps(response_object, indent=2)

    try:
        final_chain = LLMChain(llm=llm, prompt=prompt, verbose=True) # Keep verbose for debugging
        
        # Use invoke
        response = final_chain.invoke({
            "query": query,
            "reasoning_text": reasoning_text
        })

        # Extract final answer text
        if isinstance(response, dict) and 'text' in response:
            answer_text = response['text'].strip()
            if answer_text:
                final_answer = answer_text
                logger.info("[Final Output] Final answer generated.")
            else:
                logger.warning("[Final Output] LLM returned empty final answer text.")
                final_answer = "Error: Failed to generate final answer. LLM returned empty text."
        elif isinstance(response, str):
            answer_text = response.strip()
            if answer_text:
                final_answer = answer_text
                logger.info("[Final Output] Final answer generated.")
            else:
                logger.warning("[Final Output] LLM returned empty final answer text.")
                final_answer = "Error: Failed to generate final answer. LLM returned empty text."
        else:
            logger.error(f"[Final Output] Unexpected response type from LLMChain: {type(response)}")
            final_answer = "Error: Final answer generation failed due to unexpected LLM response format."

        response_object["answer"] = final_answer
        
    except Exception as e:
        logger.error(f"[Final Output] Error during final answer LLM call: {e}", exc_info=True)
        response_object["answer"] = f"Error: Failed to generate final answer. Details: {type(e).__name__}"

    return json.dumps(response_object, indent=2) 